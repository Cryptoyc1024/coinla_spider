# -*- coding: utf-8 -*-"""自定义日志类实现功能：    1. log 格式里加入了爬虫的名字    2. 爬虫运行结束统计运行数据和时长    3. error 自动写入文件    4. 忽略指定关键词的异常    5. 接入 sentry在 scrapy settings 里开关EXTENSIONS = {    'coinla_spider.loggers.Logger': 1,}"""import loggingimport osimport pprintimport refrom copy import deepcopyfrom datetime import datetimefrom functools import reducefrom logging.handlers import RotatingFileHandlerfrom raven import Clientfrom scrapy import signalsfrom twisted.internet.defer import inlineCallbacksfrom coinla_spider.databases.dbops import CacheOperatorfrom .settings import IGNORE_EXCclass Logger(object):    @classmethod    def from_crawler(cls, crawler):        return cls(crawler)    def __init__(self, crawler):        self._cache = CacheOperator(poolsize=10, reconnect=True)        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)        self.settings = crawler.settings        self.stats = crawler.stats        self.crawler = crawler        current_url = os.path.dirname(__file__)        self.parent_url = os.path.abspath(os.path.join(current_url, os.pardir))        self.root_logger = logging.getLogger()        self.init_err_out_handler()        if crawler.settings.getbool('ERR_TRACK_ENABLED') is True:            self.init_err_track_handler()    def init_err_out_handler(self):        fh = ExcFileHandler(os.path.join(self.parent_url, 'errors.log'),                            maxBytes=64 * 1024 * 1024, backupCount=1)        fh.setLevel(logging.ERROR)        self.root_logger.addHandler(fh)    def init_debug_out_handler(self, spider):        dir_url = os.path.join(self.parent_url, 'debugs')        if os.path.exists(dir_url) is False:            os.mkdir(dir_url)        dfh = RotatingFileHandler(os.path.join(dir_url, '{}.log'.format(spider.name)),                                  maxBytes=256 * 1024 * 1024, backupCount=3)        dfh.setLevel(logging.DEBUG)        self.root_logger.addHandler(dfh)    def init_err_track_handler(self):        eh = ExceptionTrackHandler(self.crawler)        eh.setLevel(logging.ERROR)        self.root_logger.addHandler(eh)    def spider_opened(self, spider):        if spider.settings.getbool('DEBUG_OUTPUT') is True:            self.init_debug_out_handler(spider)        self.time_logger = self.get_time_logger()        self.format_logger(spider)    @inlineCallbacks    def spider_closed(self, spider, **kwargs):        try:            self.clear_stats()            yield self.count_log(spider)            yield self._cache.close()        except:            pass    @staticmethod    def get_time_logger():        time_logger = logging.getLogger('TimeLogger')        sh = logging.StreamHandler()        time_logger.addHandler(sh)        return time_logger    def format_logger(self, spider):        crawl_mode = getattr(spider, 'crawl_mode', '')        start_page = getattr(spider, 'start_page', '')        end_page = getattr(spider, 'end_page', '')        mode_print = '-Mode:{}'.format(crawl_mode) if crawl_mode else ''        page_print = '-Page:{},{}-'.format(start_page, end_page) if start_page else '-'        fmt = logging.Formatter(            '%(asctime)s [{}{}%(process)d] %(message)s'.format(spider.name, mode_print + page_print),            '%Y-%m-%d %H:%M:%S')        for handler in self.root_logger.handlers + self.time_logger.handlers:            handler.setFormatter(fmt)    @inlineCallbacks    def count_log(self, spider):        crawl_mode = getattr(spider, 'crawl_mode', '')        start_page = getattr(spider, 'start_page', '')        end_page = getattr(spider, 'end_page', '')        key_tail = spider.name + crawl_mode + str(start_page) + str(end_page)        run_time = (self.stats.get_value('finish_time', datetime.now())                    - self.stats.get_value('start_time', datetime.now())).total_seconds()        avg_time = yield self._get_avg_time(key_tail)        page_crawled = self.stats.get_value('response_received_count', 0)        self.time_logger.debug(            'Run {} sec (avg {} sec), Crawled {} pages, Scraped {} items'.format(                round(run_time, 2),                round(avg_time, 2),                page_crawled,                self.stats.get_value('item_scraped_count', 0)            )        )        if run_time > 0:            yield self._cache.save(                'CrawlTime', key_tail, run_time, command='lpush', max_len=100)    @inlineCallbacks    def _get_avg_time(self, key_tail):        array = yield self._cache.load('CrawlTime', key_tail, 0, -1, command='lrange')        if not array:            return 0        total = reduce(lambda x, y: float(x) + float(y), array)        return total / len(array)    def clear_stats(self):        stats_tmp = deepcopy(self.stats._stats)        for k, v in self.stats._stats.items():            if type(v) in [list, set, dict]:                stats_tmp.pop(k)        self.stats._stats = stats_tmpclass ExceptionTrackHandler(logging.Handler):    def __init__(self, crawler):        super().__init__()        crawler.signals.connect(self.spider_closed, signal=signals.spider_opened)        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)        self.sentry = Client(dsn=crawler.settings['SENTRY_DSN'],                             environment=crawler.settings['PROJECT_ENV'],                             site='scrapy', raise_send_errors=True)        self.stats = crawler.stats        self.exc_ratio = crawler.settings['HTTP_EXC_RATIO']    def emit(self, record):        if record.exc_info:            self.sentry.captureException(record.exc_info)        else:            self.sentry.captureMessage(record.getMessage())    def filter(self, record):        return ignore_exc(record)    def spider_opened(self, spider):        self.sentry.site = spider.name    def spider_closed(self, spider):        self.track_http_err(spider)    def track_http_err(self, spider):        page_crawled = self.stats.get_value('downloader/request_count', None)        if page_crawled is None:            return None        http_exc = 0        for k, v in self.stats.get_stats().items():            if 'IgnoreRequest' in k:                continue            kw = re.search(r'(response_status_count/[45])|(type_count/\w+)', k)            if kw is not None:                http_exc += v        retry = self.stats.get_value('retry/count', 0)        err_count = http_exc - retry        # 若只出错少于3次则忽略，减少偶然情况        if err_count >= 3 and err_count / page_crawled >= self.exc_ratio:            stats_fmt = pprint.pformat(self.stats._stats, indent=4)            self.sentry.captureMessage(                'Too many invalid responses\n{}'.format(stats_fmt), level='fatal')class ExcFileHandler(RotatingFileHandler):    def __init__(self, *a, **kw):        super().__init__(*a, **kw)    def filter(self, record):        return ignore_exc(record)def ignore_exc(record):    exc_msg = format_exc(record)    for kw in IGNORE_EXC:        if kw in exc_msg:            return False    return http_exc(record)def http_exc(record):    exc_msg = format_exc(record)    for kw in ['Error downloading', 'User timeout',               'TxMongo']:        if kw in exc_msg:            return False    return Truedef format_exc(record):    err_name = record.exc_info[0].__name__ if record.exc_info else record.levelname    info = record.exc_info[1] if record.exc_info else record.getMessage()    return '{}: {}'.format(err_name, info)